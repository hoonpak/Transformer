{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "import info\n",
    "from data import CustomDataset, collate_fn\n",
    "from layers import EmbeddingWithPosition, EncoderLayer, DecoderLayer\n",
    "from sublayers import MultiHeadAttention, LayerLorm, PositionWiseFeedForward\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"../data/ende_WMT14_Tokenizer.json\"\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data tokenizing & loading: 2737it [00:00, 4671.54it/s]\n"
     ]
    }
   ],
   "source": [
    "src_train_data_path = \"../data/test/test_en.txt\"\n",
    "tgt_train_data_path = \"../data/test/test_de.txt\"\n",
    "training_dataset = CustomDataset(tokenizer=tokenizer, src_path=src_train_data_path, tgt_path=tgt_train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=training_dataset, batch_size=6, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_data, tgt_data, src_len, tgt_len = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 58])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 58])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = info.base_hyper_params['d_model']\n",
    "shared_parameter = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model, padding_idx=info.PAD)\n",
    "emb_layer = EmbeddingWithPosition(vocab_size=vocab_size, pos_max_len=info.max_len, embedding_dim=info.base_hyper_params['d_model'],\n",
    "                                  drop_rate=info.base_hyper_params['dropout_rate'], shared_parameter=shared_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = emb_layer(src_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(head=8, d_model=d_model, d_k=info.base_hyper_params['d_k'], d_v=info.base_hyper_params['d_v'], is_masked=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(x):\n",
    "    mask = torch.zeros(x.shape).to(info.device)\n",
    "    mask[x != 0] = 1. #N, L -> padding = 0, others = 1\n",
    "    mask_output = (torch.bmm(mask.unsqueeze(2), mask.unsqueeze(1)) == 0) #N, L, L\n",
    "    return mask, mask_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_mask, mask_info = get_mask(src_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = mha(emb, emb, emb, mask_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_connection = attn+emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -8.1076,   8.7598,  36.4360,  ...,  -5.2853,  -8.7302,  36.0597],\n",
       "        [-13.8747,   5.2363,  11.0384,  ...,  18.6591, -14.5472,  49.6891],\n",
       "        [ 14.3876,  -3.1392, -10.8425,  ...,  12.7769, -41.5699, -22.4045],\n",
       "        ...,\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residual_connection[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_layer = LayerLorm(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_input = ln_layer(residual_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.inner_layer = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.outer_layer = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x, mask_info):\n",
    "        \"\"\"\n",
    "        **INPUT SHAPE**\n",
    "        x -> N, L, d_model\n",
    "        \"\"\"\n",
    "        mask_info = (mask_info == 0)\n",
    "        inner_output = self.relu(self.inner_layer(x).masked_fill(mask_info.unsqueeze(-1), 0))\n",
    "        outer_output = self.outer_layer(inner_output).masked_fill(mask_info.unsqueeze(-1), 0) #N, L, d_model\n",
    "        return outer_output\n",
    "    \n",
    "    def initialization(self):\n",
    "        nn.init.xavier_uniform_(self.inner_layer.weight)\n",
    "        nn.init.xavier_uniform_(self.outer_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PWFFN_layer = PositionWiseFeedForward(d_model=d_model, d_ff=info.base_hyper_params['d_ff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_output = PWFFN_layer(ff_input, pad_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(42.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pad_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0397, -0.1271, -0.0488,  ...,  0.0168,  0.0755, -0.1220],\n",
       "        [ 0.0492, -0.1153,  0.1711,  ..., -0.2641,  0.1518,  0.1332],\n",
       "        [-0.2595, -0.0150,  0.1485,  ...,  0.0032,  0.2474,  0.1573],\n",
       "        ...,\n",
       "        [ 0.2332,  0.0788, -0.0571,  ...,  0.0133, -0.1447,  0.1605],\n",
       "        [ 0.0944, -0.1248,  0.0224,  ..., -0.1638, -0.3185, -0.0559],\n",
       "        [ 0.2455,  0.3135,  0.0048,  ...,  0.1833, -0.0470,  0.1489]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_output[0][:42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0397, -0.1271, -0.0488,  ...,  0.0168,  0.0755, -0.1220],\n",
       "        [ 0.0492, -0.1153,  0.1711,  ..., -0.2641,  0.1518,  0.1332],\n",
       "        [-0.2595, -0.0150,  0.1485,  ...,  0.0032,  0.2474,  0.1573],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_output.masked_fill((ff_input == 0), 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
